{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vihrao/MLandAI/blob/main/Copy_of_A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c76588d",
      "metadata": {
        "collapsed": true,
        "id": "6c76588d"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch\n",
        "!pip install --upgrade torchtext --force-reinstall --no-cache-dir\n",
        "!pip install transformers==4.28.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f12c49-a8b7-48bf-a11c-a5eccaf1bc1e",
      "metadata": {
        "id": "42f12c49-a8b7-48bf-a11c-a5eccaf1bc1e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "#import torchtext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we remove stop words. Print text rows. Note the mode is untrained with 50% labeled as 0 and another 50% labeled 1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1VokQ_m6mfEd"
      },
      "id": "1VokQ_m6mfEd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f140a4",
      "metadata": {
        "id": "93f140a4"
      },
      "outputs": [],
      "source": [
        "##cleaning up the text\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopWords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "def CleanText(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-z]\", \" \", text)\n",
        "    words = [word for word in text.split() if word not in stopWords]\n",
        "    text = \" \".join(words)\n",
        "    return text\n",
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/IMDB_Dataset.csv\")\n",
        "print(df.head())\n",
        "\n",
        "df[\"review\"]=df.apply(lambda row: CleanText(row[\"review\"]), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a93587-e246-4e3f-8e99-1a89cced5e47",
      "metadata": {
        "id": "26a93587-e246-4e3f-8e99-1a89cced5e47"
      },
      "outputs": [],
      "source": [
        "review_length=df['review'].str.split().str.len()\n",
        "review_length.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd0ff87-aad3-4d8e-b6f3-2b28bc92dda4",
      "metadata": {
        "id": "1cd0ff87-aad3-4d8e-b6f3-2b28bc92dda4"
      },
      "source": [
        "The average length of a reviews is 122, the shortest review length is 3 and the longest is 1442.\n",
        "\n",
        "Therefore we need to pad the sequences to equal length.\n",
        "\n",
        "The following padding function performs padding if the length of a sequence is less than the threshold and truncate the ones which are longer than threshold to the threshold.  We choose the threshold to be 256 tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IZRuSxcag095"
      },
      "id": "IZRuSxcag095"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b26e7f-2306-4b54-88e6-e44818a0bfe0",
      "metadata": {
        "id": "23b26e7f-2306-4b54-88e6-e44818a0bfe0"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = \"[PAD]\"\n",
        "df[\"token_ids\"] = df[\"review\"].apply(lambda x: tokenizer.encode(x, padding='max_length', truncation=True, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e827930-df10-4935-abd5-68c7ac33aa0a",
      "metadata": {
        "id": "5e827930-df10-4935-abd5-68c7ac33aa0a"
      },
      "outputs": [],
      "source": [
        "data = df[\"token_ids\"]\n",
        "labels = df.sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8acebb1c",
      "metadata": {
        "id": "8acebb1c"
      },
      "outputs": [],
      "source": [
        "x_train_val,x_test,y_train_val,y_test = train_test_split(data,labels,stratify=labels,test_size=0.20, random_state=42)\n",
        "x_train,x_val,y_train,y_val = train_test_split(x_train_val,y_train_val,stratify=y_train_val,test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7857c870",
      "metadata": {
        "id": "7857c870"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape,x_val.shape,x_test.shape)\n",
        "print(y_train.shape,y_val.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47be8d5",
      "metadata": {
        "id": "d47be8d5"
      },
      "source": [
        "## now you have your training, validation and test set ready, you can implement the networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer to map word index indo dense vector to capture menaing and context.\n",
        "Will help in traninng phase to learn embedding weights where similar words in context will ahve similar vectors\n",
        "Pass these through nn.Embedding(vocab_size, embedding_dim).\n",
        "\n",
        "1.   Maps integers to dense vectors, where each tolken becomes a vector of size embedding_dim 128. Embedding space can capture meaning/similarity.\n",
        "2.   Create a simple model and test it with preprocessed tokenzied reviews\n",
        "3.   Print input and output shape\n",
        "4.   print raw logits: unprocessed outputs from nn final layer before softmax activation\n",
        "5.   Compute softmax output (see formulat below)\n",
        "6.   Compare it with actual labels"
      ],
      "metadata": {
        "id": "MdtAaTapg2rm"
      },
      "id": "MdtAaTapg2rm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
        "Example 1 raw logits: tensor([[2.5000, 0.5000]])\n",
        "\n",
        "After softmax: tensor([[0.8808, 0.1192]])\n",
        "\n",
        "Calculate e^(x_i) for each logit:\n",
        "\n",
        "$$e^{2.5} \\approx 12.1825$$\n",
        "$$e^{0.5} \\approx 1.6487$$\n",
        "12.1825 + 1.6487 = 13.8312\n",
        "\n",
        "\n",
        "Divide each $$e^{x_i}$$ by the sum:\n",
        "\n",
        "For class 0: 12.1825 / 13.8312 ≈ 0.8808 (88.08%)\n",
        "\n",
        "For class 1: 1.6487 / 13.8312 ≈ 0.1192 (11.92%)"
      ],
      "metadata": {
        "id": "16fKs4k1t31J"
      },
      "id": "16fKs4k1t31J"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "UqMEdyaUkCYF"
      },
      "id": "UqMEdyaUkCYF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple vanilla NN layer\n",
        "Option 1: embedding layer and output layer\n",
        "embedding layer maps the each input word to embedding_dim vector (64 or 128 or 256), which gives out single vector for the sentence after averaging each embedding_dim vector of each word\n",
        "output layer maps the output embedding_dim vector to 2 outputs and softmax on 2 outputs to predict one class 0 or 1\n",
        "Tokens → Embedding → Average → Linear Layer → 2 Outputs\n",
        "\n",
        "Option 2: embedding layer, hidden layer, output layer\n",
        "Adds a hidden layer option with hidden embedding_dim vector 128 or 256 nodes with ReLu activation provides a embedding_dim vector as the case may be 128 or 256 which sfotmax via output layer 2 outputs and a class prediction 0 or 1\n",
        "Tokens → Embedding → Average → Hidden Linear Layer → ReLU → Output Linear Layer → 2 Outputs\n"
      ],
      "metadata": {
        "id": "tSbnd2Et2MDK"
      },
      "id": "tSbnd2Et2MDK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class SimpleTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=None, dropout_rate=0.5, num_classes=2, pad_idx=0):\n",
        "        super(SimpleTextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        if hidden_dim:\n",
        "            # Version with a hidden layer\n",
        "            self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "            self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        else:\n",
        "            # Simple version without hidden layer\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "            self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]\n",
        "        x = x.mean(dim=1)  # Average embeddings across sequence length\n",
        "\n",
        "        if self.hidden_dim:\n",
        "            # With hidden layer\n",
        "            x = self.fc1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.dropout(x)\n",
        "            out = self.fc2(x)\n",
        "        else:\n",
        "            # Without hidden layer\n",
        "            x = self.dropout(x)\n",
        "            out = self.fc(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "# Function to train and evaluate model with early stopping\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20, patience=3):\n",
        "    # Initialize best validation loss and patience counter\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Initialize history for plotting\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Time: {time_elapsed:.2f}s')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Hyperparameter optimization function\n",
        "def optimize_hyperparameters(x_train, y_train, x_val, y_val, vocab_size, pad_token_id):\n",
        "    # Convert to tensors\n",
        "    print(\"Converting data to tensors...\")\n",
        "\n",
        "    # Debug the input data\n",
        "    print(f\"x_train type: {type(x_train)}\")\n",
        "    print(f\"y_train type: {type(y_train)}\")\n",
        "\n",
        "    # Let's properly handle conversion of data to tensors\n",
        "    try:\n",
        "        # If x_train is pandas Series\n",
        "        if hasattr(x_train, 'values'):\n",
        "            x_train_tensor = torch.tensor(list(x_train.values))\n",
        "        else:\n",
        "            x_train_tensor = torch.tensor(list(x_train))\n",
        "\n",
        "        # Handle y_train conversion based on its type\n",
        "        if hasattr(y_train, 'iloc') and isinstance(y_train.iloc[0], str):\n",
        "            y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "        elif hasattr(y_train, 'values'):\n",
        "            y_train_tensor = torch.tensor(y_train.values)\n",
        "        else:\n",
        "            y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "        # Similar logic for validation data\n",
        "        if hasattr(x_val, 'values'):\n",
        "            x_val_tensor = torch.tensor(list(x_val.values))\n",
        "        else:\n",
        "            x_val_tensor = torch.tensor(list(x_val))\n",
        "\n",
        "        if hasattr(y_val, 'iloc') and isinstance(y_val.iloc[0], str):\n",
        "            y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "        elif hasattr(y_val, 'values'):\n",
        "            y_val_tensor = torch.tensor(y_val.values)\n",
        "        else:\n",
        "            y_val_tensor = torch.tensor(y_val)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting data to tensors: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Print tensor shapes for debugging\n",
        "    print(f\"x_train_tensor shape: {x_train_tensor.shape}\")\n",
        "    print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
        "    print(f\"x_val_tensor shape: {x_val_tensor.shape}\")\n",
        "    print(f\"y_val_tensor shape: {y_val_tensor.shape}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    print(\"Creating datasets and dataloaders...\")\n",
        "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # In Hyperparameter Grid we test both options:\n",
        "    ## Option 1 = NOne, Option 2 = 128/256\n",
        "    # Define hyperparameter grid\n",
        "    param_grid = {\n",
        "        'embedding_dim': [64, 128, 256],\n",
        "        'hidden_dim': [None, 128, 256],\n",
        "        'dropout_rate': [0.3, 0.5],\n",
        "        'learning_rate': [0.001, 0.0005]\n",
        "    }\n",
        "\n",
        "    # Grid search\n",
        "    best_val_acc = 0\n",
        "    best_params = None\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Starting hyperparameter optimization...\")\n",
        "    print(\"Grid:\", param_grid)\n",
        "\n",
        "    # Try all combinations\n",
        "    for embedding_dim in param_grid['embedding_dim']:\n",
        "        for hidden_dim in param_grid['hidden_dim']:\n",
        "            for dropout_rate in param_grid['dropout_rate']:\n",
        "                for learning_rate in param_grid['learning_rate']:\n",
        "                    print(f\"\\nTrying: embedding_dim={embedding_dim}, hidden_dim={hidden_dim}, \"\n",
        "                          f\"dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
        "\n",
        "                    # Create model\n",
        "                    model = SimpleTextClassifier(\n",
        "                        vocab_size=vocab_size,\n",
        "                        embedding_dim=embedding_dim,\n",
        "                        hidden_dim=hidden_dim,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        pad_idx=pad_token_id\n",
        "                    ).to(device)\n",
        "\n",
        "                    # Create optimizer\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "                    # Train for a few epochs to see how it performs\n",
        "                    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "                    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "                    print(f\"Quick evaluation - Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "                    # Check if this is the best configuration\n",
        "                    if val_acc > best_val_acc:\n",
        "                        best_val_acc = val_acc\n",
        "                        best_params = {\n",
        "                            'embedding_dim': embedding_dim,\n",
        "                            'hidden_dim': hidden_dim,\n",
        "                            'dropout_rate': dropout_rate,\n",
        "                            'learning_rate': learning_rate\n",
        "                        }\n",
        "\n",
        "    print(\"\\nBest hyperparameters:\")\n",
        "    print(best_params)\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('simple_model_performance.png')\n",
        "    plt.show()\n",
        "\n",
        "# Main function to run the entire process\n",
        "def run_simple_text_classifier(x_train, y_train, x_val, y_val, x_test, y_test, vocab_size, pad_token_id):\n",
        "    print(\"Starting Simple Text Classifier training...\")\n",
        "\n",
        "    # First, optimize hyperparameters\n",
        "    best_params = optimize_hyperparameters(x_train, y_train, x_val, y_val, vocab_size, pad_token_id)\n",
        "\n",
        "    # Prepare data for full training\n",
        "    print(\"Preparing data for full training...\")\n",
        "\n",
        "    # Convert to tensors with proper error handling\n",
        "    try:\n",
        "        # If x_train is pandas Series\n",
        "        if hasattr(x_train, 'values'):\n",
        "            x_train_tensor = torch.tensor(list(x_train.values))\n",
        "        else:\n",
        "            x_train_tensor = torch.tensor(list(x_train))\n",
        "\n",
        "        # Handle y_train conversion based on its type\n",
        "        if hasattr(y_train, 'iloc') and isinstance(y_train.iloc[0], str):\n",
        "            y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "        elif hasattr(y_train, 'values'):\n",
        "            y_train_tensor = torch.tensor(y_train.values)\n",
        "        else:\n",
        "            y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "        # Similar logic for validation and test data\n",
        "        if hasattr(x_val, 'values'):\n",
        "            x_val_tensor = torch.tensor(list(x_val.values))\n",
        "        else:\n",
        "            x_val_tensor = torch.tensor(list(x_val))\n",
        "\n",
        "        if hasattr(y_val, 'iloc') and isinstance(y_val.iloc[0], str):\n",
        "            y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "        elif hasattr(y_val, 'values'):\n",
        "            y_val_tensor = torch.tensor(y_val.values)\n",
        "        else:\n",
        "            y_val_tensor = torch.tensor(y_val)\n",
        "\n",
        "        if hasattr(x_test, 'values'):\n",
        "            x_test_tensor = torch.tensor(list(x_test.values))\n",
        "        else:\n",
        "            x_test_tensor = torch.tensor(list(x_test))\n",
        "\n",
        "        if hasattr(y_test, 'iloc') and isinstance(y_test.iloc[0], str):\n",
        "            y_test_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_test])\n",
        "        elif hasattr(y_test, 'values'):\n",
        "            y_test_tensor = torch.tensor(y_test.values)\n",
        "        else:\n",
        "            y_test_tensor = torch.tensor(y_test)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting data to tensors: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Print tensor shapes for debugging\n",
        "    print(f\"x_train_tensor shape: {x_train_tensor.shape}\")\n",
        "    print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
        "    print(f\"x_train_tensor dtype: {x_train_tensor.dtype}\")\n",
        "    print(f\"y_train_tensor dtype: {y_train_tensor.dtype}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Create model with best hyperparameters\n",
        "    model = SimpleTextClassifier(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=best_params['embedding_dim'],\n",
        "        hidden_dim=best_params['hidden_dim'],\n",
        "        dropout_rate=best_params['dropout_rate'],\n",
        "        pad_idx=pad_token_id\n",
        "    ).to(device)\n",
        "\n",
        "    # Create optimizer and criterion\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nTraining final model with best hyperparameters...\")\n",
        "    model, history = train_model(model, train_loader, val_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_history(history)\n",
        "\n",
        "    return model, history, test_acc, best_params\n",
        "\n",
        "# Define your tokenizer information\n",
        "vocab_size = 50257  # GPT2 vocab size\n",
        "pad_token_id = 0    # Adjust based on your tokenizer\n",
        "\n",
        "# Run the simple text classifier\n",
        "model, history, test_acc, best_params = run_simple_text_classifier(\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test, vocab_size, pad_token_id\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Complete!\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Example usage\n",
        "#if __name__ == \"__main__\":\n",
        "    # This code will run when the script is executed directly\n",
        "#    pass\n"
      ],
      "metadata": {
        "id": "teZcKCqZ7_6M"
      },
      "id": "teZcKCqZ7_6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Layer"
      ],
      "metadata": {
        "id": "roBOTbbdD9GF"
      },
      "id": "roBOTbbdD9GF"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# RNN Model for text classification\n",
        "class RNNTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, num_layers=1,\n",
        "                 bidirectional=False, dropout_rate=0.5, num_classes=2, pad_idx=0):\n",
        "        super(RNNTextClassifier, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Determine output size based on bidirectional setting\n",
        "        rnn_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(rnn_output_dim, num_classes)\n",
        "\n",
        "        # Save parameters for reference\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Get batch size\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Embedding layer\n",
        "        x = self.embedding(input_ids)  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # RNN layer\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
        "                         batch_size, self.hidden_dim).to(input_ids.device)\n",
        "\n",
        "        # Forward pass through RNN\n",
        "        _, hidden = self.rnn(x, h0)\n",
        "\n",
        "        # Get the final layer's hidden state\n",
        "        if self.bidirectional:\n",
        "            # Concatenate the final forward and backward hidden states\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Just use the final hidden state\n",
        "            hidden = hidden[-1,:,:]\n",
        "\n",
        "        # Output layer\n",
        "        x = self.dropout(hidden)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Store predictions and targets for classification report\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    return epoch_loss / total, correct / total, all_preds, all_targets\n",
        "\n",
        "# Function to train and evaluate model with early stopping\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20, patience=3):\n",
        "    # Initialize best validation loss and patience counter\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Initialize history for plotting\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Time: {time_elapsed:.2f}s')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rnn_model_performance.png')\n",
        "    plt.show()\n",
        "\n",
        "  #### TARIN and TEST ##########\n",
        "  # Set parameters for model\n",
        "vocab_size = 50257  # GPT2 vocab size\n",
        "pad_token_id = 0    # Default padding token ID\n",
        "\n",
        "# Create tensors from your data\n",
        "# Make sure to run your data preparation code first\n",
        "x_train_tensor = torch.tensor(list(x_train.values))\n",
        "x_val_tensor = torch.tensor(list(x_val.values))\n",
        "x_test_tensor = torch.tensor(list(x_test.values))\n",
        "\n",
        "# Convert labels\n",
        "if isinstance(y_train.iloc[0], str):\n",
        "    y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "    y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "    y_test_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_test])\n",
        "else:\n",
        "    y_train_tensor = torch.tensor(y_train.values)\n",
        "    y_val_tensor = torch.tensor(y_val.values)\n",
        "    y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = RNNTextClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=1,\n",
        "    bidirectional=True,\n",
        "    dropout_rate=0.5,\n",
        "    pad_idx=pad_token_id\n",
        ").to(device)\n",
        "\n",
        "# Create optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "print(\"Training RNN model...\")\n",
        "model, history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_targets, test_preds, target_names=['negative', 'positive']))\n",
        "\n",
        "# Plot training history\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "KvScrDeeEBMk"
      },
      "id": "KvScrDeeEBMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM Layer"
      ],
      "metadata": {
        "id": "PA_MjM4i3v9Y"
      },
      "id": "PA_MjM4i3v9Y"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# LSTM Model for text classification\n",
        "class LSTMTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, num_layers=1,\n",
        "                 bidirectional=False, dropout_rate=0.5, num_classes=2, pad_idx=0):\n",
        "        super(LSTMTextClassifier, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Determine output size based on bidirectional setting\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(lstm_output_dim, num_classes)\n",
        "\n",
        "        # Save parameters for reference\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Get batch size\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Embedding layer\n",
        "        x = self.embedding(input_ids)  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # LSTM layer\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
        "                         batch_size, self.hidden_dim).to(input_ids.device)\n",
        "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
        "                         batch_size, self.hidden_dim).to(input_ids.device)\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        _, (hidden, _) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Get the final layer's hidden state\n",
        "        if self.bidirectional:\n",
        "            # Concatenate the final forward and backward hidden states\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Just use the final hidden state\n",
        "            hidden = hidden[-1,:,:]\n",
        "\n",
        "        # Output layer\n",
        "        x = self.dropout(hidden)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Store predictions and targets for classification report\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    return epoch_loss / total, correct / total, all_preds, all_targets\n",
        "\n",
        "# Function to train and evaluate model with early stopping\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20, patience=3):\n",
        "    # Initialize best validation loss and patience counter\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Initialize history for plotting\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Time: {time_elapsed:.2f}s')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Hyperparameter optimization function for LSTM\n",
        "def optimize_lstm_hyperparameters(x_train, y_train, x_val, y_val, vocab_size, pad_token_id):\n",
        "    print(\"Converting data to tensors...\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    x_train_tensor = torch.tensor(list(x_train.values))\n",
        "\n",
        "    # Convert string labels to numeric if needed\n",
        "    if isinstance(y_train.iloc[0], str):\n",
        "        y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "        y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "    else:\n",
        "        y_train_tensor = torch.tensor(y_train.values)\n",
        "        y_val_tensor = torch.tensor(y_val.values)\n",
        "\n",
        "    x_val_tensor = torch.tensor(list(x_val.values))\n",
        "\n",
        "    # Print tensor shapes\n",
        "    print(f\"x_train_tensor shape: {x_train_tensor.shape}\")\n",
        "    print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Define hyperparameter grid - use a smaller grid to save time\n",
        "    param_grid = {\n",
        "        'embedding_dim': [128],\n",
        "        'hidden_dim': [128, 256],\n",
        "        'num_layers': [1],\n",
        "        'bidirectional': [False, True],\n",
        "        'dropout_rate': [0.5],\n",
        "        'learning_rate': [0.001]\n",
        "    }\n",
        "\n",
        "    # Grid search\n",
        "    best_val_acc = 0\n",
        "    best_params = None\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Starting hyperparameter optimization...\")\n",
        "    print(\"Grid:\", param_grid)\n",
        "\n",
        "    # Try a subset of combinations to save time\n",
        "    for embedding_dim in param_grid['embedding_dim']:\n",
        "        for hidden_dim in param_grid['hidden_dim']:\n",
        "            for num_layers in param_grid['num_layers']:\n",
        "                for bidirectional in param_grid['bidirectional']:\n",
        "                    for dropout_rate in param_grid['dropout_rate']:\n",
        "                        for learning_rate in param_grid['learning_rate']:\n",
        "                            print(f\"\\nTrying: embedding_dim={embedding_dim}, hidden_dim={hidden_dim}, \"\n",
        "                                  f\"num_layers={num_layers}, bidirectional={bidirectional}, \"\n",
        "                                  f\"dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
        "\n",
        "                            # Create model\n",
        "                            model = LSTMTextClassifier(\n",
        "                                vocab_size=vocab_size,\n",
        "                                embedding_dim=embedding_dim,\n",
        "                                hidden_dim=hidden_dim,\n",
        "                                num_layers=num_layers,\n",
        "                                bidirectional=bidirectional,\n",
        "                                dropout_rate=dropout_rate,\n",
        "                                pad_idx=pad_token_id\n",
        "                            ).to(device)\n",
        "\n",
        "                            # Create optimizer\n",
        "                            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "                            # Quick evaluation (1 epoch)\n",
        "                            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "                            val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "                            print(f\"Quick evaluation - Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "                            # Check if this is the best configuration\n",
        "                            if val_acc > best_val_acc:\n",
        "                                best_val_acc = val_acc\n",
        "                                best_params = {\n",
        "                                    'embedding_dim': embedding_dim,\n",
        "                                    'hidden_dim': hidden_dim,\n",
        "                                    'num_layers': num_layers,\n",
        "                                    'bidirectional': bidirectional,\n",
        "                                    'dropout_rate': dropout_rate,\n",
        "                                    'learning_rate': learning_rate\n",
        "                                }\n",
        "\n",
        "    print(\"\\nBest hyperparameters:\")\n",
        "    print(best_params)\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lstm_model_performance.png')\n",
        "    plt.show()\n",
        "########## TRAIN and TEST ##########\n",
        "\n",
        "try:\n",
        "    print(f\"x_train shape: {x_train.shape}\")\n",
        "    print(f\"y_train shape: {y_train.shape}\")\n",
        "except NameError:\n",
        "    print(\"Data variables not found. Let's load them again.\")\n",
        "\n",
        "    # Re-load your data if needed\n",
        "    # This is assuming you have the data already processed somewhere\n",
        "    # If not, you'll need to run your data processing code again\n",
        "\n",
        "    # For example:\n",
        "    # df = pd.read_csv(\"/content/IMDB_Dataset.csv\")\n",
        "    # ...process data as before...\n",
        "\n",
        "# Set padding token ID\n",
        "# Since you used GPT2 tokenizer with \"[PAD]\" as padding token\n",
        "pad_token_id = 0  # Default padding token ID, typically 0 in most tokenizers\n",
        "vocab_size = 50257  # GPT2 vocab size\n",
        "\n",
        "# Convert to tensors\n",
        "x_train_tensor = torch.tensor(list(x_train.values))\n",
        "x_val_tensor = torch.tensor(list(x_val.values))\n",
        "x_test_tensor = torch.tensor(list(x_test.values))\n",
        "\n",
        "# Convert labels\n",
        "if y_train.iloc[0] == 'positive' or y_train.iloc[0] == 'negative':\n",
        "    # String labels\n",
        "    y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "    y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "    y_test_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_test])\n",
        "else:\n",
        "    # Numeric labels\n",
        "    y_train_tensor = torch.tensor(y_train.values)\n",
        "    y_val_tensor = torch.tensor(y_val.values)\n",
        "    y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define model parameters\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "bidirectional = True\n",
        "num_layers = 1\n",
        "dropout_rate = 0.5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create model\n",
        "model = LSTMTextClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    dropout_rate=dropout_rate,\n",
        "    pad_idx=pad_token_id\n",
        ").to(device)\n",
        "\n",
        "# Create optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "print(\"Training LSTM model...\")\n",
        "model, history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_targets, test_preds, target_names=['negative', 'positive']))\n",
        "\n",
        "# Plot training history\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "n0lE4e9t6k8e"
      },
      "id": "n0lE4e9t6k8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCiZsbRr8ABU"
      },
      "id": "aCiZsbRr8ABU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GRU Layer"
      ],
      "metadata": {
        "id": "BmphfeYeEevc"
      },
      "id": "BmphfeYeEevc"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# GRU Model for text classification\n",
        "class GRUTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, num_layers=1,\n",
        "                 bidirectional=False, dropout_rate=0.5, num_classes=2, pad_idx=0):\n",
        "        super(GRUTextClassifier, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Determine output size based on bidirectional setting\n",
        "        gru_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(gru_output_dim, num_classes)\n",
        "\n",
        "        # Save parameters for reference\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Get batch size\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Embedding layer\n",
        "        x = self.embedding(input_ids)  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # GRU layer\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
        "                         batch_size, self.hidden_dim).to(input_ids.device)\n",
        "\n",
        "        # Forward pass through GRU\n",
        "        _, hidden = self.gru(x, h0)\n",
        "\n",
        "        # Get the final layer's hidden state\n",
        "        if self.bidirectional:\n",
        "            # Concatenate the final forward and backward hidden states\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            # Just use the final hidden state\n",
        "            hidden = hidden[-1,:,:]\n",
        "\n",
        "        # Output layer\n",
        "        x = self.dropout(hidden)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Store predictions and targets for classification report\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    return epoch_loss / total, correct / total, all_preds, all_targets\n",
        "\n",
        "# Function to train and evaluate model with early stopping\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20, patience=3):\n",
        "    # Initialize best validation loss and patience counter\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Initialize history for plotting\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Time: {time_elapsed:.2f}s')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gru_model_performance.png')\n",
        "    plt.show()\n",
        "\n",
        "##### TRAIN and TEST #######\n",
        "# Set parameters for model\n",
        "vocab_size = 50257  # GPT2 vocab size\n",
        "pad_token_id = 0    # Default padding token ID\n",
        "\n",
        "# Create tensors from your data\n",
        "# Make sure to run your data preparation code first\n",
        "x_train_tensor = torch.tensor(list(x_train.values))\n",
        "x_val_tensor = torch.tensor(list(x_val.values))\n",
        "x_test_tensor = torch.tensor(list(x_test.values))\n",
        "\n",
        "# Convert labels\n",
        "if isinstance(y_train.iloc[0], str):\n",
        "    y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "    y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "    y_test_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_test])\n",
        "else:\n",
        "    y_train_tensor = torch.tensor(y_train.values)\n",
        "    y_val_tensor = torch.tensor(y_val.values)\n",
        "    y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = GRUTextClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=1,\n",
        "    bidirectional=True,\n",
        "    dropout_rate=0.5,\n",
        "    pad_idx=pad_token_id\n",
        ").to(device)\n",
        "\n",
        "# Create optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "print(\"Training GRU model...\")\n",
        "model, history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_targets, test_preds, target_names=['negative', 'positive']))\n",
        "\n",
        "# Plot training history\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "KLLZ4JWSExvi"
      },
      "id": "KLLZ4JWSExvi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer with positional embedding"
      ],
      "metadata": {
        "id": "s2iGAF2ZFZqW"
      },
      "id": "s2iGAF2ZFZqW"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Positional Encoding for Transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension [1, max_seq_len, d_model]\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a parameter but should be saved with model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to embedding\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Multi-head Self-Attention Block\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.output = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Linear projections and reshape\n",
        "        q = self.query(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.key(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.value(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back and apply output projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.output(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Feed-Forward Network Block\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear1(x)))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        attn_output = self.self_attn(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len=5000, dropout=0.1, pad_idx=0):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Stack of encoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create padding mask\n",
        "        padding_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Embedding with positional encoding\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Apply encoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Transformer Text Classifier\n",
        "class TransformerTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=8, num_layers=2,\n",
        "                 d_ff=512, max_seq_len=5000, dropout=0.1, num_classes=2, pad_idx=0):\n",
        "        super(TransformerTextClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_encoder = TransformerEncoder(\n",
        "            vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout, pad_idx\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get transformer encoder output\n",
        "        encoder_output = self.transformer_encoder(x)\n",
        "\n",
        "        # Global average pooling over sequence dimension\n",
        "        pooled_output = encoder_output.mean(dim=1)\n",
        "\n",
        "        # Apply dropout and classification layer\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Store predictions and targets for classification report\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    return epoch_loss / total, correct / total, all_preds, all_targets\n",
        "\n",
        "# Function to train and evaluate model with early stopping\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20, patience=3):\n",
        "    # Initialize best validation loss and patience counter\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Initialize history for plotting\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Time: {time_elapsed:.2f}s')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transformer_model_performance.png')\n",
        "    plt.show()\n",
        "\n",
        "#### TRAIN and TEST #####\n",
        "# Set parameters for model\n",
        "vocab_size = 50257  # GPT2 vocab size\n",
        "pad_token_id = 0    # Default padding token ID\n",
        "\n",
        "# Create tensors from your data\n",
        "# Make sure to run your data preparation code first\n",
        "x_train_tensor = torch.tensor(list(x_train.values))\n",
        "x_val_tensor = torch.tensor(list(x_val.values))\n",
        "x_test_tensor = torch.tensor(list(x_test.values))\n",
        "\n",
        "# Convert labels\n",
        "if isinstance(y_train.iloc[0], str):\n",
        "    y_train_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_train])\n",
        "    y_val_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_val])\n",
        "    y_test_tensor = torch.tensor([1 if label == 'positive' else 0 for label in y_test])\n",
        "else:\n",
        "    y_train_tensor = torch.tensor(y_train.values)\n",
        "    y_val_tensor = torch.tensor(y_val.values)\n",
        "    y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 32  # Smaller batch size for transformer due to memory requirements\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = TransformerTextClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,        # Embedding dimension\n",
        "    num_heads=8,        # Number of attention heads\n",
        "    num_layers=2,       # Number of transformer layers\n",
        "    d_ff=512,           # Feed-forward dimension\n",
        "    dropout=0.1,\n",
        "    num_classes=2,\n",
        "    pad_idx=pad_token_id\n",
        ").to(device)\n",
        "\n",
        "# Create optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower learning rate for transformer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Transformer model...\")\n",
        "model, history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_targets, test_preds, target_names=['negative', 'positive']))\n",
        "\n",
        "# Plot training history\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "Z3UWf8G5FUOM"
      },
      "id": "Z3UWf8G5FUOM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JjfytngMF9eq"
      },
      "id": "JjfytngMF9eq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}